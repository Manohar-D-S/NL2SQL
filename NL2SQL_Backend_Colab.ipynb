{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ NL2SQL Backend Server with BART Model\n",
        "\n",
        "This notebook runs a FastAPI backend server that translates natural language to SQL using the BART model.\n",
        "\n",
        "## üìã What This Does\n",
        "\n",
        "1. ‚úÖ Installs all required dependencies\n",
        "2. ‚úÖ Loads the BART NL2SQL model (SwastikM/bart-large-nl2sql)\n",
        "3. ‚úÖ Starts a FastAPI server\n",
        "4. ‚úÖ Exposes it via ngrok tunnel (publicly accessible)\n",
        "5. ‚úÖ Provides API endpoint for your frontend to use\n",
        "\n",
        "## ‚ö° Quick Start\n",
        "\n",
        "1. **Runtime ‚Üí Change runtime type** ‚Üí Select **GPU (T4)**\n",
        "2. **Run all cells** (Runtime ‚Üí Run all)\n",
        "3. **Copy the ngrok URL** from the output\n",
        "4. **Use it in your frontend** to connect to the backend\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Step 1: Install Dependencies"
      ],
      "metadata": {
        "id": "step1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q fastapi uvicorn transformers torch pyngrok pydantic"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü§ñ Step 2: Load BART Model\n",
        "\n",
        "This will download the model (~1.6GB) - takes about 1-2 minutes on first run."
      ],
      "metadata": {
        "id": "step2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "print(\"üîÑ Loading BART NL2SQL model...\")\n",
        "model_name = \"SwastikM/bart-large-nl2sql\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Move to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"‚úÖ Model loaded successfully on {device.upper()}!\")\n",
        "print(f\"   GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üåê Step 3: Create FastAPI Server"
      ],
      "metadata": {
        "id": "step3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, HTTPException\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "from typing import List\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "\n",
        "# Allow nested event loops (required for Colab)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Create FastAPI app\n",
        "app = FastAPI(\n",
        "    title=\"NL2SQL BART API\",\n",
        "    description=\"Natural Language to SQL translation using BART\",\n",
        "    version=\"1.0.0\"\n",
        ")\n",
        "\n",
        "# Enable CORS for frontend\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # Allow all origins (adjust for production)\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Request/Response models\n",
        "class TranslateRequest(BaseModel):\n",
        "    natural_language: str\n",
        "    database: str = \"default\"\n",
        "    schema_context: str = \"\"\n",
        "\n",
        "class SQLCandidate(BaseModel):\n",
        "    sql: str\n",
        "    confidence: float\n",
        "    reasoning: str\n",
        "\n",
        "class TranslateResponse(BaseModel):\n",
        "    candidates: List[SQLCandidate]\n",
        "    database: str\n",
        "\n",
        "def translate_to_sql(natural_language: str, schema_context: str = \"\") -> List[SQLCandidate]:\n",
        "    \"\"\"Translate natural language to SQL using BART\"\"\"\n",
        "    try:\n",
        "        # Format prompt\n",
        "        if schema_context:\n",
        "            prompt = f\"sql_prompt: {natural_language}\\nsql_context: {schema_context}\"\n",
        "        else:\n",
        "            prompt = f\"sql_prompt: {natural_language}\"\n",
        "        \n",
        "        # Tokenize\n",
        "        inputs = tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=512,\n",
        "            truncation=True\n",
        "        ).input_ids.to(device)\n",
        "        \n",
        "        # Generate SQL with beam search\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                inputs,\n",
        "                max_new_tokens=200,\n",
        "                num_beams=3,\n",
        "                num_return_sequences=3,\n",
        "                do_sample=False,\n",
        "                early_stopping=True,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True,\n",
        "            )\n",
        "        \n",
        "        # Decode results\n",
        "        candidates = []\n",
        "        sequences = outputs.sequences\n",
        "        scores = outputs.sequences_scores if hasattr(outputs, 'sequences_scores') else None\n",
        "        \n",
        "        for idx, sequence in enumerate(sequences):\n",
        "            sql = tokenizer.decode(sequence, skip_special_tokens=True)\n",
        "            \n",
        "            # Calculate confidence\n",
        "            if scores is not None:\n",
        "                confidence = float(torch.exp(scores[idx]).cpu())\n",
        "            else:\n",
        "                confidence = max(0.5, 1.0 - (idx * 0.15))\n",
        "            \n",
        "            candidates.append(\n",
        "                SQLCandidate(\n",
        "                    sql=sql.strip(),\n",
        "                    confidence=round(confidence, 3),\n",
        "                    reasoning=f\"Generated by BART model (beam {idx + 1})\"\n",
        "                )\n",
        "            )\n",
        "        \n",
        "        # Remove duplicates\n",
        "        unique_candidates = []\n",
        "        seen_sql = set()\n",
        "        for candidate in candidates:\n",
        "            if candidate.sql not in seen_sql:\n",
        "                unique_candidates.append(candidate)\n",
        "                seen_sql.add(candidate.sql)\n",
        "        \n",
        "        return unique_candidates\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error in translation: {e}\")\n",
        "        return []\n",
        "\n",
        "# API Endpoints\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\n",
        "        \"message\": \"NL2SQL BART API\",\n",
        "        \"status\": \"running\",\n",
        "        \"model\": \"SwastikM/bart-large-nl2sql\",\n",
        "        \"device\": device,\n",
        "        \"endpoints\": {\n",
        "            \"translate\": \"/api/translate/\",\n",
        "            \"health\": \"/api/health\",\n",
        "            \"docs\": \"/docs\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "@app.get(\"/api/health\")\n",
        "async def health():\n",
        "    return {\"status\": \"healthy\", \"model_loaded\": True, \"device\": device}\n",
        "\n",
        "@app.post(\"/api/translate/\", response_model=TranslateResponse)\n",
        "async def translate(request: TranslateRequest):\n",
        "    \"\"\"Translate natural language to SQL\"\"\"\n",
        "    if not request.natural_language or not request.natural_language.strip():\n",
        "        raise HTTPException(status_code=400, detail=\"Query cannot be empty\")\n",
        "    \n",
        "    candidates = translate_to_sql(request.natural_language, request.schema_context)\n",
        "    \n",
        "    if not candidates:\n",
        "        raise HTTPException(status_code=500, detail=\"Translation failed\")\n",
        "    \n",
        "    return TranslateResponse(\n",
        "        candidates=candidates,\n",
        "        database=request.database\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ FastAPI server created!\")"
      ],
      "metadata": {
        "id": "create_server"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Step 4: Start Server with ngrok Tunnel\n",
        "\n",
        "**IMPORTANT:** Copy the ngrok URL from the output below!"
      ],
      "metadata": {
        "id": "step4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "\n",
        "# Set your ngrok auth token (get free token from https://ngrok.com)\n",
        "# Optional but recommended to avoid session limits\n",
        "NGROK_AUTH_TOKEN = \"\"  # Paste your token here (optional)\n",
        "\n",
        "if NGROK_AUTH_TOKEN:\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Start server in background thread\n",
        "def run_server():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "\n",
        "thread = threading.Thread(target=run_server, daemon=True)\n",
        "thread.start()\n",
        "\n",
        "# Create ngrok tunnel\n",
        "import time\n",
        "time.sleep(2)  # Wait for server to start\n",
        "\n",
        "public_url = ngrok.connect(8000)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üéâ BACKEND SERVER IS RUNNING!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nüì° Public URL: {public_url}\")\n",
        "print(f\"\\nüîó API Endpoints:\")\n",
        "print(f\"   - Translation: {public_url}/api/translate/\")\n",
        "print(f\"   - Health: {public_url}/api/health\")\n",
        "print(f\"   - Docs: {public_url}/docs\")\n",
        "print(f\"\\nüí° Usage in Frontend:\")\n",
        "print(f\"   Update your frontend to use: {public_url}\")\n",
        "print(f\"\\n‚ö†Ô∏è  Keep this notebook running to keep the server alive!\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Keep the cell running\n",
        "import time\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(60)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nüõë Server stopped\")"
      ],
      "metadata": {
        "id": "start_server"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ Step 5: Test the API (Optional)\n",
        "\n",
        "Run this cell to test if the translation works:"
      ],
      "metadata": {
        "id": "step5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Get the public URL from ngrok\n",
        "tunnels = ngrok.get_tunnels()\n",
        "if tunnels:\n",
        "    api_url = str(tunnels[0].public_url)\n",
        "    \n",
        "    # Test translation\n",
        "    test_query = \"Show all students with marks above 80\"\n",
        "    \n",
        "    print(f\"üß™ Testing translation: '{test_query}'\\n\")\n",
        "    \n",
        "    response = requests.post(\n",
        "        f\"{api_url}/api/translate/\",\n",
        "        json={\n",
        "            \"natural_language\": test_query,\n",
        "            \"database\": \"students_db\",\n",
        "            \"schema_context\": \"CREATE TABLE students (id INT, name VARCHAR(100), marks INT);\"\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        print(\"‚úÖ Translation successful!\\n\")\n",
        "        print(\"SQL Candidates:\")\n",
        "        for i, candidate in enumerate(data['candidates'], 1):\n",
        "            print(f\"\\n{i}. {candidate['sql']}\")\n",
        "            print(f\"   Confidence: {candidate['confidence']:.2%}\")\n",
        "    else:\n",
        "        print(f\"‚ùå Error: {response.status_code}\")\n",
        "        print(response.text)\n",
        "else:\n",
        "    print(\"‚ùå No tunnels found. Make sure Step 4 is running!\")"
      ],
      "metadata": {
        "id": "test_api"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
