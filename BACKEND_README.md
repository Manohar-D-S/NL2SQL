# NL2SQL Backend with BART Model

This backend provides a FastAPI server that translates natural language queries into SQL using either:
- **Local BART Model** (SwastikM/bart-large-nl2sql) - No API key needed
- **OpenAI GPT** - Requires API key

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Configure Environment

Create a `.env` file in the project root:

```bash
# Use local BART model (recommended for development)
MODEL_MODE=local

# Database configuration (adjust as needed)
DATABASE_URL=postgresql://postgres:password@localhost:5432/nl2sql_db
READ_ONLY_DB_USER=true
MAX_EXECUTION_MS=10000
LOG_LEVEL=INFO
```

### 3. Run the Backend

```bash
# Development mode with auto-reload
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# Or production mode
uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 4
```

The backend will be available at: **http://localhost:8000**

## ğŸ“¡ API Endpoints

### Translate Natural Language to SQL

**POST** `/api/translate/`

```json
{
  "natural_language": "Show all students with marks above 80",
  "database": "students_db"
}
```

Response:
```json
{
  "candidates": [
    {
      "sql": "SELECT * FROM students WHERE marks > 80;",
      "confidence": 0.95,
      "reasoning": "Generated by BART model (beam 1)"
    }
  ],
  "schema_context_version": "v1",
  "database": "students_db"
}
```

### Other Endpoints

- **GET** `/` - API information
- **GET** `/docs` - Interactive API documentation (Swagger UI)
- **GET** `/api/health` - Health check
- **POST** `/api/execute/` - Execute SQL query
- **POST** `/api/validate/` - Validate SQL safety
- **POST** `/api/explain/` - Get SQL explanation
- **POST** `/api/optimize/` - Get optimization suggestions

## ğŸ”§ Configuration Options

### Using Local BART Model (Default)

```env
MODEL_MODE=local
```

**Pros:**
- No API costs
- Works offline
- Fast inference
- Privacy-friendly

**Cons:**
- Requires ~2GB RAM for model
- First load takes 10-30 seconds
- Slightly less accurate than GPT

### Using OpenAI API

```env
MODEL_MODE=openai
OPENAI_API_KEY=sk-your-api-key-here
```

**Pros:**
- More accurate
- Better reasoning
- No local RAM usage

**Cons:**
- Requires API key
- Costs per request
- Requires internet

## ğŸ§ª Testing the API

### Using curl

```bash
curl -X POST "http://localhost:8000/api/translate/" \
  -H "Content-Type: application/json" \
  -d '{
    "natural_language": "Get all users from the database",
    "database": "default"
  }'
```

### Using Python

```python
import requests

response = requests.post(
    "http://localhost:8000/api/translate/",
    json={
        "natural_language": "Find students with GPA above 3.5",
        "database": "students_db"
    }
)

print(response.json())
```

### Using the Frontend

The Next.js frontend automatically connects to `http://localhost:8000` and uses all API endpoints.

## ğŸ“Š How It Works

1. **User sends natural language query** via the frontend
2. **Backend receives request** at `/api/translate/`
3. **Schema context is fetched** from the database (cached for performance)
4. **BART model generates SQL:**
   - Formats prompt: `sql_prompt: <query> sql_context: <schema>`
   - Uses beam search to generate 3 candidate SQL queries
   - Ranks by confidence score
5. **Candidates returned** to frontend for user selection

## ğŸ› ï¸ Model Information

**Model:** [SwastikM/bart-large-nl2sql](https://huggingface.co/SwastikM/bart-large-nl2sql)
- Architecture: BART (Bidirectional and Auto-Regressive Transformers)
- Training: Fine-tuned on SQL generation tasks
- Size: ~1.6GB
- Performance: Good for common SQL patterns

## ğŸ“ Development

### Project Structure

```
app/
â”œâ”€â”€ main.py                 # FastAPI app entry point
â”œâ”€â”€ api/
â”‚   â””â”€â”€ routes/
â”‚       â”œâ”€â”€ translate.py    # Translation endpoint
â”‚       â”œâ”€â”€ execute.py      # SQL execution
â”‚       â””â”€â”€ ...
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ bart_translator.py  # NEW: BART model service
â”‚   â”œâ”€â”€ llm_translator.py   # Translator orchestrator
â”‚   â”œâ”€â”€ schema_service.py   # Schema management
â”‚   â””â”€â”€ ...
â””â”€â”€ core/
    â”œâ”€â”€ config.py           # Configuration
    â””â”€â”€ logger.py           # Logging setup
```

### Running Tests

```bash
pytest tests/
```

## ğŸ› Troubleshooting

### Model Download Issues

The first time you run the server, it will download the BART model (~1.6GB):

```
Downloading tokenizer...
Downloading model...
Model loaded successfully
```

This is normal and only happens once.

### Out of Memory

If you get OOM errors, the model is trying to load on GPU but there's insufficient VRAM. The service will automatically fall back to CPU.

### Connection Refused

Make sure the backend is running on port 8000 before starting the frontend.

## ğŸ“„ License

MIT License - See LICENSE file for details
