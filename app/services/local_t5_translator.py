"""Local T5 model for NL→SQL translation (offline mode)"""

import logging
import asyncio
from typing import List, Optional
import torch
from transformers import T5ForConditionalGeneration, T5Tokenizer

from app.services.llm_translator import TranslationCandidate

logger = logging.getLogger(__name__)


class LocalT5Translator:
    """T5 small model for local NL→SQL translation"""
    
    def __init__(self, model_path: str = "./models/t5-small"):
        self.model_path = model_path
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = None
        self.tokenizer = None
        
        try:
            logger.info(f"Loading T5 model from {model_path} on {self.device}")
            self.tokenizer = T5Tokenizer.from_pretrained("t5-small")
            self.model = T5ForConditionalGeneration.from_pretrained("t5-small").to(self.device)
            logger.info("T5 model loaded successfully")
        except Exception as e:
            logger.error(f"Failed to load T5 model: {e}")
            self.model = None
    
    async def translate(
        self,
        natural_language: str,
        schema_context: str,
        num_candidates: int = 3,
    ) -> List[TranslationCandidate]:
        """Translate NL to SQL using T5"""
        
        if not self.model:
            logger.error("T5 model not loaded")
            return []
        
        try:
            # Format input for T5
            input_text = f"translate English to SQL: {schema_context}\n\n{natural_language}"
            
            # Run in thread pool to avoid blocking
            loop = asyncio.get_event_loop()
            candidates = await loop.run_in_executor(
                None,
                self._generate_candidates,
                input_text,
                num_candidates,
            )
            
            return candidates
        
        except Exception as e:
            logger.error(f"T5 translation error: {e}")
            return []
    
    def _generate_candidates(
        self,
        input_text: str,
        num_candidates: int = 3,
    ) -> List[TranslationCandidate]:
        """Generate SQL candidates (runs in thread)"""
        
        try:
            # Tokenize input
            inputs = self.tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # Generate multiple candidates
            outputs = self.model.generate(
                **inputs,
                max_length=256,
                num_beams=num_candidates,
                num_return_sequences=min(num_candidates, 3),
                temperature=0.7,
                do_sample=False,
            )
            
            # Decode outputs
            candidates = []
            for i, output in enumerate(outputs):
                sql = self.tokenizer.decode(output, skip_special_tokens=True).strip()
                
                # Ensure SQL ends with semicolon
                if sql and not sql.endswith(";"):
                    sql += ";"
                
                # Calculate confidence based on beam score (approximate)
                confidence = 0.85 - (i * 0.1)  # Decrease confidence for less likely beams
                
                candidates.append(TranslationCandidate(
                    sql=sql,
                    confidence=max(0.5, confidence),
                    reasoning=f"Generated by T5 beam {i+1}",
                ))
            
            return candidates
        
        except Exception as e:
            logger.error(f"Candidate generation error: {e}")
            return []
