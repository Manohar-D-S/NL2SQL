# ğŸ¯ Complete Setup Guide: NL2SQL Application

## What We Built

Your application now has:
1. **Frontend**: Next.js app running on `http://localhost:3000`
2. **Backend**: FastAPI server with BART model for NL2SQL translation on `http://localhost:8000`

## ğŸ“‹ Backend Features

The backend (`app/` directory) includes:

### âœ… BART Model Integration
- **File**: `app/services/bart_translator.py`
- **Model**: SwastikM/bart-large-nl2sql (Hugging Face)
- **Features**:
  - Singleton pattern (loads model once)
  - GPU support (falls back to CPU)
  - Beam search for multiple SQL candidates
  - Confidence scoring

### âœ… Updated LLM Translator
- **File**: `app/services/llm_translator.py`  
- **Modes**:
  - `local`: Uses BART model (no API key needed)
  - `openai`: Uses OpenAI GPT (requires API key)

### âœ… FastAPI Endpoints
- `POST /api/translate/` - Convert NL to SQL
- `POST /api/execute/` - Execute SQL queries
- `POST /api/validate/` - Validate SQL safety
- `POST /api/explain/` - Explain SQL queries
- `POST /api/optimize/` - Get optimization suggestions
- `GET /docs` - Interactive API documentation

## ğŸš€ How to Run

### Step 1: Create Environment Configuration

Create a `.env` file in `d:\dbms\`:

```env
MODEL_MODE=local
DATABASE_URL=postgresql://postgres:password@localhost:5432/nl2sql_db
READ_ONLY_DB_USER=true
MAX_EXECUTION_MS=10000
LOG_LEVEL=INFO
```

### Step 2: Install Backend Dependencies

```bash
pip install -r requirements.txt
```

### Step 3: Start the Backend

**Option A: Using the startup script**
```powershell
.\start-backend.ps1
```

**Option B: Manual command**
```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

**âš ï¸ Important Notes:**
- First startup will download the BART model (~1.6GB) - this takes 1-2 minutes
- Model is cached after first download
- Server will be at `http://localhost:8000`
- API docs at `http://localhost:8000/docs`

### Step 4: Frontend is Already Running

Your frontend is already running on `http://localhost:3000` from the earlier `npm start` command!

## ğŸ§ª Testing

### Test 1: Verify BART Model Works

Run the test script:
```bash
python test_bart.py
```

Expected output:
```
ğŸ§ª BART Model Translation Test
======================================================================

ğŸ“ Test 1/5
Query: Show all students with marks above 80
----------------------------------------------------------------------
âœ… Generated 3 SQL candidates:

  1. SQL: SELECT * FROM students WHERE marks > 80;
     Confidence: 95.00%
     Reasoning: Generated by BART model (beam 1)
...
```

### Test 2: Test Backend API

With backend running, open a new terminal:

```bash
curl -X POST "http://localhost:8000/api/translate/" \
  -H "Content-Type: application/json" \
  -d "{\"natural_language\": \"Show all students\", \"database\": \"students_db\"}"
```

### Test 3: Test Full Application

1. Open browser: `http://localhost:3000`
2. Type a natural language query: "Show all students with marks above 80"
3. Click "Translate"
4. See SQL candidates generated by BART model!

## ğŸ“Š Architecture Flow

```
User Types Query
     â†“
Next.js Frontend (http://localhost:3000)
     â†“
POST /api/translate/ 
     â†“
FastAPI Backend (http://localhost:8000)
     â†“
LLMTranslator Service
     â†“
BARTTranslator Service
     â†“
BART Model (SwastikM/bart-large-nl2sql)
     â†“
SQL Candidates
     â†“
Response to Frontend
     â†“
User Selects & Executes SQL
```

## ğŸ”„ Switching Between Models

### Use Local BART Model (Default)
```env
MODEL_MODE=local
```
- âœ… Free, no API costs
- âœ… Works offline
- âœ… Privacy-friendly
- âš ï¸ Requires ~2GB RAM

### Use OpenAI API
```env
MODEL_MODE=openai
OPENAI_API_KEY=sk-your-key-here
```
- âœ… More accurate
- âœ… Better reasoning
- âš ï¸ Costs per request
- âš ï¸ Requires internet

## ğŸ“ File Structure

```
d:\dbms\
â”œâ”€â”€ app/                          # Backend (FastAPI)
â”‚   â”œâ”€â”€ main.py                   # Entry point
â”‚   â”œâ”€â”€ api/routes/
â”‚   â”‚   â””â”€â”€ translate.py          # Translation endpoint
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ bart_translator.py    # ğŸ†• BART model service
â”‚   â”‚   â”œâ”€â”€ llm_translator.py     # Updated with BART
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ core/
â”‚       â””â”€â”€ config.py             # Configuration
â”‚
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ example.py                # Original BART example
â”‚
â”œâ”€â”€ test_bart.py                  # ğŸ†• Test script
â”œâ”€â”€ start-backend.ps1             # ğŸ†• Startup script
â”œâ”€â”€ BACKEND_README.md             # ğŸ†• Backend documentation
â”œâ”€â”€ ENV_CONFIG.md                 # ğŸ†• Environment config guide
â”œâ”€â”€ requirements.txt              # Python dependencies
â”‚
â”œâ”€â”€ app/ (Next.js)                # Frontend
â”œâ”€â”€ components/
â”œâ”€â”€ package.json
â””â”€â”€ ...
```

## ğŸ› Troubleshooting

### Backend won't start
```bash
# Check if port 8000 is in use
netstat -ano | findstr :8000

# Kill the process if needed
taskkill /PID <pid> /F
```

### Model download fails
- Check internet connection
- Ensure you have ~2GB free disk space
- Try running `python test_bart.py` first

### Frontend can't connect to backend
- Verify backend is running on port 8000
- Check CORS settings in `app/main.py`
- Verify frontend is making requests to `http://localhost:8000`

### Out of Memory
- BART model requires ~2GB RAM
- Close other applications
- Or switch to `MODEL_MODE=openai`

## ğŸ“š Next Steps

1. **Test the integration** - Run both servers and test with queries
2. **Customize prompts** - Edit `bart_translator.py` to tune the model
3. **Add more features** - Extend the API with new endpoints
4. **Deploy** - Use Docker for production deployment

## ğŸ’¡ Tips

- Use the interactive API docs at `http://localhost:8000/docs` to test endpoints
- Monitor backend logs to see model loading and inference
- First query after startup takes longer (model initialization)
- Subsequent queries are fast (~100-500ms)

---

**Need help?** Check the logs or run `python test_bart.py` to diagnose issues.
